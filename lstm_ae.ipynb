{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb9253d",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM) Autoecoder (AE) Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d3cdf7",
   "metadata": {},
   "source": [
    "## How to run a Juypter notebook\n",
    "\n",
    "https://docs.jupyter.org/en/latest/running.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ebef32",
   "metadata": {},
   "source": [
    "## Including dependencies\n",
    "\n",
    "To run this, you'll need matplotlib, seaborn, torch, sklearn, numpy, and pandas. Pip should handle these for you, but torch can be tricky. Google is your friend here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ad5085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "# sklean\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Misc\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b2cda",
   "metadata": {},
   "source": [
    "## Setting our execution device\n",
    "\n",
    "This variable allows the use of the GPU (if avaliable). This only works with NVIDIA GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a19b9c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6281d650",
   "metadata": {},
   "source": [
    "### Encoder Class\n",
    "\n",
    "Below is the class that contains the encoder part of our network. It is a 2 layer encoder that uses two LSTM (RNNs) as its layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ba828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "  def __init__(self, seq_len, n_features, embedding_dim=64):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.seq_len, self.n_features = seq_len, n_features\n",
    "    self.embedding_dim, self.hidden_dim = embedding_dim, 2 * embedding_dim\n",
    "\n",
    "    self.rnn1 = nn.LSTM(\n",
    "      input_size=n_features,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "    \n",
    "    self.rnn2 = nn.LSTM(\n",
    "      input_size=self.hidden_dim,\n",
    "      hidden_size=embedding_dim,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.reshape((1, self.seq_len, self.n_features))   # (batch_size, seq_len, num of features)\n",
    "    # print(x)\n",
    "    x, (hidden_n, cell_n) = self.rnn1(x)\n",
    "    x, (hidden_n, cell_n) = self.rnn2(x)\n",
    "\n",
    "    return hidden_n.reshape((1, self.embedding_dim))  # num of units/neurons into the hidden layer\n",
    "  \n",
    "  # passing in results from the encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a77a6e",
   "metadata": {},
   "source": [
    "### Decoder Class\n",
    "\n",
    "Below is the class that contains the decoder part of our network. It is a 2 layer encoder that uses two LSTM (RNNs) as its layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83c14d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "  def __init__(self, seq_len, input_dim=64, n_features=1):\n",
    "\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.seq_len, self.input_dim = seq_len, input_dim\n",
    "\n",
    "    self.hidden_dim, self.n_features = 2 * input_dim, n_features\n",
    "\n",
    "    self.rnn1 = nn.LSTM(\n",
    "      input_size=input_dim,\n",
    "      hidden_size=input_dim,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "\n",
    "    self.rnn2 = nn.LSTM(\n",
    "      input_size=input_dim,\n",
    "      hidden_size=self.hidden_dim,\n",
    "      num_layers=1,\n",
    "      batch_first=True\n",
    "    )\n",
    "\n",
    "    self.output_layer = nn.Linear(self.hidden_dim, n_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    x = x.repeat(self.seq_len, self.n_features)\n",
    "    x = x.reshape((self.n_features, self.seq_len, self.input_dim))\n",
    "\n",
    "    x, (hidden_n, cell_n) = self.rnn1(x)\n",
    "    x, (hidden_n, cell_n) = self.rnn2(x)\n",
    "\n",
    "    x = x.reshape((self.seq_len, self.hidden_dim))\n",
    "\n",
    "    return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b64b9c6",
   "metadata": {},
   "source": [
    "### LSTM-Autoencoder class\n",
    "\n",
    "Now, we can combine our encoder and decoder networks into a single LSTM-AE. Notice, in this class, the forward function only makes calls to the encoder then the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "658c9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentAutoencoder(nn.Module):\n",
    "\n",
    "  def __init__(self, seq_len, n_features=1, embedding_dim=64):\n",
    "    super(RecurrentAutoencoder, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n",
    "    self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)\n",
    "    x = self.decoder(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be24ff3d",
   "metadata": {},
   "source": [
    "## Reading in our synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0afa9208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0       0.1         1         2         3         4         5  \\\n",
      "0    normal -0.348796 -0.085726  0.309447  0.597268  0.601409  0.712171   \n",
      "1    normal  0.034529 -0.006321  0.075302  0.303938  0.564451  0.624538   \n",
      "2    normal -0.098415  0.140292  0.223849  0.194160  0.469268  0.791196   \n",
      "3    normal  0.088932 -0.013717  0.388148  0.433235  0.302517  0.441382   \n",
      "4    normal  0.253196 -0.099384  0.083056  0.197967  0.432012  0.094493   \n",
      "..      ...       ...       ...       ...       ...       ...       ...   \n",
      "995  normal -0.275241  0.232920  0.405765  0.404851  0.533835  0.256463   \n",
      "996  normal  0.219121  0.040536  0.009555  0.117296  0.501165  0.609651   \n",
      "997  normal -0.237998  0.070051  0.353420  0.560810  0.505321  0.162667   \n",
      "998  normal -0.189628 -0.078464  0.160853  0.502834  0.182108  0.902421   \n",
      "999  normal  0.084698  0.267118  0.065285 -0.023995  0.233047  0.318685   \n",
      "\n",
      "            6         7         8  ...       140       141       142  \\\n",
      "0    0.646340  0.627614  0.563835  ...  1.062057  1.000586  0.539058   \n",
      "1    0.509215  0.502546  1.057680  ...  0.799740  0.720664  0.731453   \n",
      "2    0.301928  0.555758  0.313708  ...  0.931150  0.772224  0.898971   \n",
      "3    0.624010  0.651953  0.737077  ...  1.188192  0.378740  1.037796   \n",
      "4    0.496609  0.849734  0.768422  ...  1.030772  0.949803  0.986839   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "995  0.212643  0.737241  1.005373  ...  1.410226  0.733997  0.556712   \n",
      "996  0.631829  0.629523  1.011458  ...  0.923974  0.721737  0.932995   \n",
      "997  0.560156  0.862573  0.567298  ...  0.945973  0.815717  1.005463   \n",
      "998  0.495025  0.971352  0.703251  ...  0.809922  0.912992  0.885567   \n",
      "999  0.627335  0.281557  1.017136  ...  0.608140  0.443529  0.712815   \n",
      "\n",
      "          143       144       145       146       147       148       149  \n",
      "0    0.574834  0.658500  0.006164  0.218059  0.251220 -0.093210 -0.162807  \n",
      "1    1.072650  0.841792  0.430162  0.599148  0.285180 -0.000307 -0.035623  \n",
      "2    0.755932  0.408136  0.573112  0.011805  0.152507 -0.033914 -0.003588  \n",
      "3    0.812570  0.777160  0.120052  0.254116  0.170000  0.378568 -0.384819  \n",
      "4    0.603974  0.421809  0.187985  0.668981 -0.021860  0.112778  0.150960  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "995  0.878238  0.174305  0.531356  0.611430  0.278991  0.024530  0.108126  \n",
      "996  0.784211  0.293793  0.645063  0.484227 -0.122540 -0.083230 -0.440576  \n",
      "997  0.792795  0.448854  0.294460  0.491743  0.400771  0.050858 -0.435406  \n",
      "998  0.407545  0.585294  0.238629  0.391475  0.087963 -0.103899 -0.071006  \n",
      "999  0.931433  0.377855  0.370389  0.409091  0.046497 -0.217586  0.019571  \n",
      "\n",
      "[1000 rows x 151 columns]\n",
      "\n",
      "Number of sequences: 1000\n",
      "Sequence length:  150\n",
      "Number of features:  1\n"
     ]
    }
   ],
   "source": [
    "# Raw data from .csv\n",
    "df = pd.read_csv('synthetic_data_normal.csv')\n",
    "print(df)\n",
    "\n",
    "# Grab the targets (anomaly labels)\n",
    "# Not needed for the LSTM-AE, but useful for further classification\n",
    "targets_raw = df.iloc[:, 0]\n",
    "targets_np = targets_raw.to_numpy()\n",
    "le = preprocessing.LabelEncoder()\n",
    "targets = le.fit_transform(targets_np)\n",
    "inputs  = torch.tensor(targets)\n",
    "\n",
    "# Grab the features (time-series)\n",
    "features_raw = df.iloc[:, 1:]\n",
    "features_np = features_raw.to_numpy()\n",
    "features_total = np.column_stack([targets,features_np])\n",
    "features = torch.Tensor(features_total)\n",
    "\n",
    "# Do a little extra work to get the number of sequences, length of each sequence, and number of features\n",
    "sequences = features_raw.astype(np.float32).to_numpy().tolist()\n",
    "dataset = [torch.tensor(s).unsqueeze(1) for s in features_np]\n",
    "n_seq, seq_len, n_features = torch.stack(dataset).shape\n",
    "\n",
    "# Print out some metrics of our dataset\n",
    "print(\"\\nNumber of sequences:\", n_seq)\n",
    "print(\"Sequence length: \", seq_len)\n",
    "print(\"Number of features: \", n_features)\n",
    "\n",
    "# Create a tensor dataset and a dataloader\n",
    "train = data_utils.TensorDataset(features, inputs)\n",
    "\n",
    "# Probably want to leave batch_size equal to 1 here...\n",
    "train_loader = data_utils.DataLoader(train, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a030c2",
   "metadata": {},
   "source": [
    "### Instantiate a model of LSTM-AE network\n",
    "\n",
    "Here, we create an object to store the network model. Then we move the model to the GPU (if avaliable). Finally, we print out the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2bb30a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecurrentAutoencoder(\n",
      "  (encoder): Encoder(\n",
      "    (rnn1): LSTM(1, 64, batch_first=True)\n",
      "    (rnn2): LSTM(64, 32, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (rnn1): LSTM(32, 32, batch_first=True)\n",
      "    (rnn2): LSTM(32, 64, batch_first=True)\n",
      "    (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# the third parameter (embedding dim) is one you may want to try increasing if you have more complex data and are getting poor results\n",
    "model = RecurrentAutoencoder(seq_len+1, n_features, 32)    # model obj\n",
    "model = model.to(device)                                   # moving model to gpu\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdabf01f",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f438a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, n_epochs):\n",
    "\n",
    "  # Try different learning rates! Does that change how well the network changes?\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr= 1e-3)\n",
    "\n",
    "  # Try different loss functions! See why (or why not) they work better\n",
    "  # Remember: The notation of Loss and Cost functions are often used interchangeably\n",
    "  criterion = nn.L1Loss(reduction='sum').to(device)     # Loss function\n",
    "                                                        # summing error; L1Loss = mean absolute error in torch\n",
    "\n",
    "  history = dict(train=[], val=[])  \n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    model = model.train()                     # Putting the model in training mode\n",
    "\n",
    "    train_losses = []                         # list to store loss values\n",
    "\n",
    "    for seq_true,_ in train_dataset:          # iterate over each seq for train data\n",
    "      optimizer.zero_grad()                   # no accumulation\n",
    "\n",
    "      seq_true = seq_true.to(device)          # putting sequence to gpu\n",
    "      seq_pred = model(seq_true)              # prediction\n",
    "\n",
    "      loss = criterion(seq_pred, seq_true.T)  # measuring error\n",
    "\n",
    "      loss.backward()                         # backprop\n",
    "      optimizer.step()                        # optimize weights\n",
    "\n",
    "      train_losses.append(loss.item())        # record loss by adding to training losses\n",
    "\n",
    "    train_loss = np.mean(train_losses)        # get the mean loss\n",
    "    history['train'].append(train_loss)       # store mean loss\n",
    "    \n",
    "    print(f'Epoch {epoch}: train loss = {train_loss}')\n",
    "\n",
    "  return history "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e08d2",
   "metadata": {},
   "source": [
    "### Train the model for n epochs\n",
    "\n",
    "An epoch in a neural network is the training of the neural network with all the training data for one cycle. Lower loss is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e8603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 141.85227192115784\n",
      "Epoch 2: train loss = 28.986126922607422\n",
      "Epoch 3: train loss = 26.219880439758303\n",
      "Epoch 4: train loss = 26.076440423965455\n",
      "Epoch 5: train loss = 26.023223121643067\n",
      "Epoch 6: train loss = 25.928776906967162\n",
      "Epoch 7: train loss = 25.595530435562132\n",
      "Epoch 8: train loss = 25.64119337463379\n",
      "Epoch 9: train loss = 25.39769501876831\n",
      "Epoch 10: train loss = 25.490263862609865\n",
      "Epoch 11: train loss = 25.396369943618776\n",
      "Epoch 12: train loss = 25.300572896957398\n",
      "Epoch 13: train loss = 25.23149860191345\n",
      "Epoch 14: train loss = 25.114116025924684\n",
      "Epoch 15: train loss = 25.055405590057372\n",
      "Epoch 16: train loss = 25.025915613174437\n",
      "Epoch 17: train loss = 24.962692499160767\n",
      "Epoch 18: train loss = 25.01837215423584\n",
      "Epoch 19: train loss = 25.05641528892517\n",
      "Epoch 20: train loss = 25.067996810913087\n",
      "Epoch 21: train loss = 24.841528827667236\n",
      "Epoch 22: train loss = 24.962525718688966\n"
     ]
    }
   ],
   "source": [
    "# More epochs take longer to train but could provide better results on more complex data or larger models\n",
    "epochs = 25\n",
    "his = train_model(model, train_loader, epochs)\n",
    "\n",
    "# Saving the model as a file\n",
    "MODEL_PATH = 'ltsm_ae_model_small.pth'\n",
    "torch.save(model, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c54e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just some plotting stuffs\n",
    "sns.set_theme()\n",
    "ax = plt.figure().gca()\n",
    "ax.plot(his['train'])\n",
    "# ax.plot(his['val'])\n",
    "plt.ylabel('Loss') \n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train'])\n",
    "# plt.legend(['train', 'test'])\n",
    "plt.title('Loss over training epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff72a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset):\n",
    "  predictions, losses = [], []\n",
    "  criterion = nn.L1Loss(reduction = 'sum').to(device) #L1 to gpu\n",
    "\n",
    "  with torch.no_grad():\n",
    "    model = model.eval()      # putting in evaluation mode          \n",
    "    for seq_true,_ in dataset:\n",
    "      seq_true = seq_true.to(device)\n",
    "      seq_pred = model(seq_true)\n",
    "\n",
    "      loss = criterion(seq_pred, seq_true.T)  # calculating loss\n",
    "      \n",
    "      predictions.append(seq_pred.cpu().numpy().flatten()) # appending predictions & loss to results \n",
    "      losses.append(loss.item())\n",
    "  return predictions, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c064425",
   "metadata": {},
   "source": [
    "### Run a prediction using the trained model and normal dataset\n",
    "\n",
    "Running a prediction means that we do a single forward propagation of the network and take the result of the output layer as the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ea88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('ltsm_ae_model.pth')     # Load model\n",
    "model = model.to(device)                    # put model on GPU\n",
    "_, losses = predict(model, train_loader)    # Get prediction, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64a65a0",
   "metadata": {},
   "source": [
    "Now we can grab a histogram of our losses. We want this so we can define a decision boundary we can use to classify what qualifies as a normal time-series. Since we ran a prediction based on the dataset we know contains only \"normal\" time-series, the threshold must contain the values within the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ccd22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of losses\n",
    "sns.histplot(\n",
    "    losses, kde=True,\n",
    "    stat=\"density\", kde_kws=dict(cut=30),\n",
    "    alpha=.4, edgecolor=(1, 1, 1, .4),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b3296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold of what we consider to be normal\n",
    "# For normal time-series, we want our losses to capture values below the threshold\n",
    "# The concept of a threshold here is a little arbitary\n",
    "# Try playing with the logic here and see if you can get a better view of the results!\n",
    "THRESHOLD = 42\n",
    "correct = sum(l <= THRESHOLD for l in losses)\n",
    "print(f'Correct normal predictions: {correct}/{len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc62076",
   "metadata": {},
   "source": [
    "### Run a prediction using the trained model and anomaly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954ec687",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('synthetic_data_high.csv')\n",
    "targets_raw = df.iloc[:, 0]\n",
    "targets_np = targets_raw.to_numpy()\n",
    "le = preprocessing.LabelEncoder()\n",
    "targets = le.fit_transform(targets_np)\n",
    "inputs  = torch.tensor(targets)\n",
    "\n",
    "features_raw = df.iloc[:, 1:]\n",
    "features_np = features_raw.to_numpy()\n",
    "features_total = np.column_stack([targets,features_np])\n",
    "features = torch.Tensor(features_total)\n",
    "\n",
    "sequences = features_raw.astype(np.float32).to_numpy().tolist()\n",
    "dataset = [torch.tensor(s).unsqueeze(1) for s in features_np]\n",
    "n_seq, seq_len, n_features = torch.stack(dataset).shape\n",
    "\n",
    "anomaly = data_utils.TensorDataset(features, inputs)\n",
    "anomaly_loader = data_utils.DataLoader(anomaly, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1664e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, losses2 = predict(model, anomaly_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec176f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    losses2, kde=True,\n",
    "    stat=\"density\", kde_kws=dict(cut=20),\n",
    "    alpha=.4, edgecolor=(1, 1, 1, .4),\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94b88e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold of what we consider to be an anomaly\n",
    "# For normal time-series, we want our losses to capture values above the threshold\n",
    "# The concept of a threshold here is a little arbitary\n",
    "# Try playing with the logic here and see if you can get a better view of the results!\n",
    "THRESHOLD2 = 46\n",
    "correct = sum(l > THRESHOLD2 for l in losses2)\n",
    "print(f'Correct anomaly precitions: {correct}/{len(anomaly_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d4b49a",
   "metadata": {},
   "source": [
    "### Overlay truth and reconstructions\n",
    "\n",
    "This plots the first 6 normal time-series + reconstructions and the first 6 anomaly time-series + reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(data, model, title, ax):\n",
    "  predictions, pred_losses = predict(model, [data])\n",
    "  \n",
    "  ax.plot(data[0][0], label= 'true')\n",
    "  ax.plot(predictions[0], label= 'predicted')\n",
    "  ax.grid()\n",
    "  ax.set_title(f'{title} (loss: {np.around(pred_losses, 2)})') # showing loss in the title with 2 decimal points\n",
    "  ax.legend()\n",
    "\n",
    "  # 1st row is 6 examples from normal heartbeat, next row represents anomalies\n",
    "fig, axs = plt.subplots(\n",
    "    nrows= 2,\n",
    "    ncols= 4,\n",
    "    sharex = True,\n",
    "    sharey = True,\n",
    "    figsize = (16, 6)\n",
    ")\n",
    "\n",
    "# looping through 6 examples\n",
    "train_iter = iter(train_loader)\n",
    "for i in range(4):\n",
    "    data_batch = next(train_iter)\n",
    "    plot_prediction(data_batch, model, title= 'Normal', ax = axs[0, i])\n",
    "    \n",
    "anomaly_iter = iter(anomaly_loader)\n",
    "for i in range(4):\n",
    "    data_batch = next(anomaly_iter)\n",
    "    plot_prediction(data_batch, model, title= 'Anomaly', ax = axs[1, i])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
